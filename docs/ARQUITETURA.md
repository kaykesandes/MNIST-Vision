# ğŸ—ï¸ Arquitetura da Rede Neural

## ğŸ“š Ãndice
- [VisÃ£o Geral](#visÃ£o-geral)
- [Arquitetura Detalhada](#arquitetura-detalhada)
- [Componentes](#componentes)
- [Fluxo de Dados](#fluxo-de-dados)
- [Justificativas de Design](#justificativas-de-design)
- [Alternativas](#alternativas)

## ğŸ¯ VisÃ£o Geral

A rede neural implementada Ã© uma **Feedforward Neural Network** (tambÃ©m conhecida como Multi-Layer Perceptron - MLP) projetada especificamente para classificaÃ§Ã£o de dÃ­gitos manuscritos do dataset MNIST.

## ğŸ—ï¸ Arquitetura Detalhada

### ğŸ“Š Estrutura Geral:
```
INPUT LAYER    HIDDEN LAYER 1    HIDDEN LAYER 2    OUTPUT LAYER
   (784)     â†’     (128)      â†’      (64)       â†’     (10)
   
28Ã—28 pixels  â†’  128 neurons   â†’   64 neurons   â†’  10 classes
               +ReLU activation  +ReLU activation  +LogSoftmax
```

### ğŸ”¢ EspecificaÃ§Ãµes NumÃ©ricas:

| Camada | Entrada | SaÃ­da | ParÃ¢metros | AtivaÃ§Ã£o |
|--------|---------|-------|------------|----------|
| Linear1 | 784 | 128 | 100,352 | ReLU |
| Linear2 | 128 | 64 | 8,192 | ReLU |
| Linear3 | 64 | 10 | 640 | LogSoftmax |
| **Total** | - | - | **109,184** | - |

### ğŸ“ CÃ¡lculo de ParÃ¢metros:

#### Camada 1 (Linear1):
```
Pesos: 784 Ã— 128 = 100,352
Bias: 128
Total: 100,352 + 128 = 100,480
```

#### Camada 2 (Linear2):
```
Pesos: 128 Ã— 64 = 8,192  
Bias: 64
Total: 8,192 + 64 = 8,256
```

#### Camada 3 (Linear3):
```
Pesos: 64 Ã— 10 = 640
Bias: 10  
Total: 640 + 10 = 650
```

#### Total de ParÃ¢metros:
```
100,480 + 8,256 + 650 = 109,386 parÃ¢metros treinÃ¡veis
```

## ğŸ§© Componentes

### 1. ğŸ“¥ Camada de Entrada (Input Layer)

**DimensÃ£o:** 784 neurÃ´nios

**Justificativa:** 
- Imagens MNIST sÃ£o 28Ã—28 pixels = 784 pixels
- Cada pixel representa um valor de intensidade (0-1)
- Achatamento da matriz 2D para vetor 1D

```python
# Preprocessamento da entrada
imagens = imagens.view(imagens.shape[0], -1)  # [batch, 28, 28] â†’ [batch, 784]
```

### 2. ğŸ§  Primeira Camada Oculta (Hidden Layer 1)

**DimensÃ£o:** 128 neurÃ´nios  
**AtivaÃ§Ã£o:** ReLU (Rectified Linear Unit)

**CaracterÃ­sticas:**
- **FunÃ§Ã£o:** ExtraÃ§Ã£o de caracterÃ­sticas de baixo nÃ­vel
- **ReLU:** `f(x) = max(0, x)` - introduz nÃ£o-linearidade
- **Vantagens do ReLU:**
  - Computacionalmente eficiente
  - Evita problema do vanishing gradient
  - Esparsidade (muitos zeros)

```python
x = F.relu(self.linear1(x))  # Aplica transformaÃ§Ã£o linear + ReLU
```

### 3. ğŸ§  Segunda Camada Oculta (Hidden Layer 2)

**DimensÃ£o:** 64 neurÃ´nios
**AtivaÃ§Ã£o:** ReLU

**CaracterÃ­sticas:**
- **FunÃ§Ã£o:** CombinaÃ§Ã£o de caracterÃ­sticas de alto nÃ­vel
- **ReduÃ§Ã£o dimensional:** 128 â†’ 64 (compressÃ£o de informaÃ§Ã£o)
- **RepresentaÃ§Ã£o abstrata:** PadrÃµes mais complexos

### 4. ğŸ“¤ Camada de SaÃ­da (Output Layer)

**DimensÃ£o:** 10 neurÃ´nios (um para cada dÃ­gito 0-9)
**AtivaÃ§Ã£o:** LogSoftmax

**CaracterÃ­sticas:**
- **LogSoftmax:** Converte logits em log-probabilidades
- **Estabilidade numÃ©rica:** Evita overflow/underflow
- **Compatibilidade:** Funciona com NLLLoss

```python
# LogSoftmax aplicado na dimensÃ£o das classes
return F.log_softmax(x, dim=1)
```

## ğŸ“Š Fluxo de Dados

### 1. **Entrada â†’ Camada 1:**
```
Input: [batch_size, 784]
TransformaÃ§Ã£o: xâ‚ = ReLU(Wâ‚x + bâ‚)
Output: [batch_size, 128]
```

### 2. **Camada 1 â†’ Camada 2:**
```
Input: [batch_size, 128]  
TransformaÃ§Ã£o: xâ‚‚ = ReLU(Wâ‚‚xâ‚ + bâ‚‚)
Output: [batch_size, 64]
```

### 3. **Camada 2 â†’ SaÃ­da:**
```
Input: [batch_size, 64]
TransformaÃ§Ã£o: xâ‚ƒ = Wâ‚ƒxâ‚‚ + bâ‚ƒ
Output: [batch_size, 10]
```

### 4. **AtivaÃ§Ã£o Final:**
```
Input: [batch_size, 10]
TransformaÃ§Ã£o: output = LogSoftmax(xâ‚ƒ)
Output: [batch_size, 10] (log-probabilidades)
```

## ğŸ¯ Justificativas de Design

### 1. **Por que Feedforward?**
- âœ… **Simplicidade:** FÃ¡cil de implementar e entender
- âœ… **EficiÃªncia:** Computacionalmente leve
- âœ… **AdequaÃ§Ã£o:** MNIST Ã© um problema relativamente simples
- âœ… **Baseline:** Boa referÃªncia para comparaÃ§Ãµes

### 2. **Por que 128 â†’ 64 neurÃ´nios?**
- ğŸ¯ **ReduÃ§Ã£o gradual:** Evita perda brusca de informaÃ§Ã£o
- ğŸ¯ **Capacidade adequada:** Suficiente para MNIST
- ğŸ¯ **EficiÃªncia:** NÃ£o Ã© excessivamente grande
- ğŸ¯ **ExperiÃªncia empÃ­rica:** Funciona bem na prÃ¡tica

### 3. **Por que ReLU?**
- âš¡ **Velocidade:** Computacionalmente eficiente
- âš¡ **Gradientes:** NÃ£o sofre vanishing gradient
- âš¡ **Esparsidade:** Ativa apenas neurÃ´nios relevantes
- âš¡ **Simplicidade:** FÃ¡cil de implementar

### 4. **Por que LogSoftmax + NLLLoss?**
- ğŸ”¢ **Estabilidade numÃ©rica:** Evita problemas com exp()
- ğŸ”¢ **EficiÃªncia:** CombinaÃ§Ã£o otimizada
- ğŸ”¢ **Gradientes:** Melhores propriedades de otimizaÃ§Ã£o

## ğŸ”„ Alternativas Consideradas

### 1. **Arquiteturas Alternativas:**

#### Mais Simples:
```
784 â†’ 64 â†’ 10
```
**Pros:** Mais rÃ¡pida, menos parÃ¢metros
**Cons:** Menor capacidade de representaÃ§Ã£o

#### Mais Complexa:
```
784 â†’ 256 â†’ 128 â†’ 64 â†’ 10
```
**Pros:** Maior capacidade
**Cons:** Overfitting potencial, mais lenta

#### CNN (Convolutional):
```
28Ã—28 â†’ Conv2D â†’ MaxPool â†’ Conv2D â†’ FC â†’ 10
```
**Pros:** Melhor para imagens, invariÃ¢ncia espacial
**Cons:** Mais complexa, desnecessÃ¡ria para MNIST

### 2. **FunÃ§Ãµes de AtivaÃ§Ã£o Alternativas:**

#### Sigmoid:
```python
x = torch.sigmoid(self.linear1(x))
```
**Pros:** SaÃ­da limitada (0,1)
**Cons:** Vanishing gradient, computacionalmente cara

#### Tanh:
```python
x = torch.tanh(self.linear1(x))
```
**Pros:** SaÃ­da centrada em zero (-1,1)
**Cons:** Vanishing gradient

#### Leaky ReLU:
```python
x = F.leaky_relu(self.linear1(x), negative_slope=0.01)
```
**Pros:** Evita "dead neurons"
**Cons:** Mais complexa

### 3. **FunÃ§Ãµes de Perda Alternativas:**

#### CrossEntropy:
```python
criterio = nn.CrossEntropyLoss()
# Requer modificaÃ§Ã£o na saÃ­da (sem LogSoftmax)
```

#### MSE (Mean Squared Error):
```python
criterio = nn.MSELoss()
# Inadequada para classificaÃ§Ã£o
```

## ğŸ“Š AnÃ¡lise de Complexidade

### **Complexidade Temporal:**
- **Forward Pass:** O(n Ã— m) onde n = batch_size, m = parÃ¢metros
- **Backward Pass:** O(n Ã— m)
- **Total por batch:** O(2nm)

### **Complexidade Espacial:**
- **ParÃ¢metros:** 109,386 Ã— 4 bytes = ~437KB
- **AtivaÃ§Ãµes:** batch_size Ã— (784 + 128 + 64 + 10) Ã— 4 bytes
- **Gradientes:** Mesmo tamanho dos parÃ¢metros

### **Performance Esperada:**
```
Dataset: 60,000 imagens de treino
Batch Size: 64
Batches por epoch: 938
Tempo por epoch: ~30-60s (CPU)
PrecisÃ£o esperada: 90%+
```

## ğŸ¨ RepresentaÃ§Ã£o Visual

### Arquitetura Completa:
```
INPUT LAYER (784)
    |
    | Wâ‚[784Ã—128] + bâ‚[128]
    â†“
HIDDEN LAYER 1 (128)
    |
    | ReLU activation
    | Wâ‚‚[128Ã—64] + bâ‚‚[64]
    â†“
HIDDEN LAYER 2 (64)
    |
    | ReLU activation  
    | Wâ‚ƒ[64Ã—10] + bâ‚ƒ[10]
    â†“
OUTPUT LAYER (10)
    |
    | LogSoftmax
    â†“
LOG-PROBABILITIES
```

### Fluxo de Gradientes (Backpropagation):
```
Loss â† NLLLoss
  â†‘
LogSoftmax â† Output Layer
  â†‘
ReLU â† Hidden Layer 2
  â†‘  
ReLU â† Hidden Layer 1
  â†‘
Input Layer
```

## ğŸ”§ ConfiguraÃ§Ãµes de Treinamento

### Otimizador:
```python
optimizer = optim.SGD(
    modelo.parameters(),
    lr=0.01,        # Taxa de aprendizado
    momentum=0.5    # Momentum para acelerar convergÃªncia
)
```

### FunÃ§Ã£o de Perda:
```python
criterion = nn.NLLLoss()  # Negative Log Likelihood
```

### HyperparÃ¢metros:
```python
BATCH_SIZE = 64
EPOCHS = 3
LEARNING_RATE = 0.01
MOMENTUM = 0.5
```

---

**ğŸ¯ Esta arquitetura foi escolhida por equilibrar simplicidade, eficiÃªncia e performance adequada para o problema MNIST.**
